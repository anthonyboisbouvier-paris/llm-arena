<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Research Paper — Multi-Agent Judging for LLM Evaluation | Agent Clash</title>
  <meta name="description" content="Research paper: Multi-Agent Judging for LLM Evaluation. A 3-judge AI panel achieves 88% concordance with humans on MT-Bench and 76% on Chatbot Arena — within human inter-annotator agreement.">
  <meta name="keywords" content="LLM evaluation, multi-agent judging, LLM-as-judge, human preference alignment, Borda count, Cohen kappa, frontier model comparison, Agent Clash research">
  <meta name="author" content="Anthony Boisbouvier">
  <link rel="canonical" href="https://agent-clash.ai/paper.html">
  <meta property="og:title" content="Research — Multi-Agent Judging for LLM Evaluation">
  <meta property="og:description" content="88% concordance with humans on large gaps, 76% on frontier models. Within human inter-annotator agreement in both cases.">
  <meta property="og:url" content="https://agent-clash.ai/paper.html">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Agent Clash">
  <link rel="icon" href="/favicon.svg" type="image/svg+xml">
  <link href="https://fonts.googleapis.com/css2?family=Cinzel:wght@700;900&family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "name": "Multi-Agent Judging for LLM Evaluation: Concordance with Human Preferences Across Capability Gaps",
    "author": { "@type": "Person", "name": "Anthony Boisbouvier" },
    "url": "https://agent-clash.ai/paper.html",
    "description": "A multi-judge evaluation framework where three frontier-class LLMs evaluate candidate models under blind conditions with Borda count aggregation."
  }
  </script>

  <style>
    *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
    body {
      font-family: 'Inter', sans-serif;
      background: #0d0a06;
      color: #d4c9a8;
      line-height: 1.8;
      min-height: 100vh;
    }
    .top-bar {
      border-bottom: 1px solid rgba(218,165,32,0.15);
      background: rgba(13,10,6,0.95);
      padding: 16px 24px;
      position: sticky; top: 0; z-index: 10;
      backdrop-filter: blur(12px);
      display: flex; align-items: center; justify-content: space-between;
    }
    .top-bar a.logo {
      font-family: 'Cinzel', serif;
      font-weight: 900;
      font-size: 20px;
      background: linear-gradient(135deg, #ffd700, #daa520, #b8860b);
      -webkit-background-clip: text; -webkit-text-fill-color: transparent;
      text-decoration: none;
    }
    .top-bar a.logo:hover { opacity: 0.8; }
    .top-bar .nav-links { display: flex; gap: 20px; }
    .top-bar .nav-links a {
      font-size: 13px; color: #8a7e60; text-decoration: none;
      transition: color 0.2s;
    }
    .top-bar .nav-links a:hover { color: #daa520; }

    .container { max-width: 900px; margin: 0 auto; padding: 40px 24px 80px; }

    /* Hero */
    .paper-hero {
      text-align: center;
      padding: 40px 0 30px;
      border-bottom: 1px solid rgba(218,165,32,0.12);
      margin-bottom: 40px;
    }
    .paper-hero .badge {
      display: inline-block;
      background: rgba(218,165,32,0.12);
      border: 1px solid rgba(218,165,32,0.25);
      border-radius: 20px;
      padding: 4px 16px;
      font-size: 11px;
      font-weight: 600;
      color: #daa520;
      text-transform: uppercase;
      letter-spacing: 1.5px;
      margin-bottom: 20px;
    }
    h1 {
      font-family: 'Cinzel', serif;
      font-size: 28px;
      font-weight: 900;
      background: linear-gradient(135deg, #ffd700, #daa520);
      -webkit-background-clip: text; -webkit-text-fill-color: transparent;
      margin-bottom: 14px;
      line-height: 1.3;
    }
    .paper-hero .author {
      font-size: 14px; color: #8a7e60; margin-bottom: 6px;
    }
    .paper-hero .author a { color: #b0a060; text-decoration: none; }
    .paper-hero .author a:hover { color: #daa520; }
    .paper-hero .venue {
      font-size: 12px; color: #6a6050; font-style: italic;
    }

    /* Bottom line box */
    .bottom-line {
      background: linear-gradient(135deg, rgba(218,165,32,0.08), rgba(184,134,11,0.06));
      border: 1px solid rgba(218,165,32,0.2);
      border-radius: 14px;
      padding: 24px 28px;
      margin: 30px 0 40px;
      text-align: center;
    }
    .bottom-line p {
      font-size: 16px; line-height: 1.7; color: #e0d5c0; margin: 0;
    }
    .bottom-line strong { color: #ffd700; }

    /* Metric cards */
    .metrics-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 16px;
      margin: 30px 0;
    }
    .metric-card {
      background: rgba(218,165,32,0.05);
      border: 1px solid rgba(218,165,32,0.12);
      border-radius: 12px;
      padding: 20px;
      text-align: center;
      transition: border-color 0.3s, transform 0.3s;
    }
    .metric-card:hover {
      border-color: rgba(218,165,32,0.35);
      transform: translateY(-2px);
    }
    .metric-card .value {
      font-family: 'Cinzel', serif;
      font-size: 36px;
      font-weight: 900;
      background: linear-gradient(135deg, #ffd700, #daa520);
      -webkit-background-clip: text; -webkit-text-fill-color: transparent;
      line-height: 1.2;
    }
    .metric-card .label {
      font-size: 12px;
      color: #8a7e60;
      margin-top: 6px;
      text-transform: uppercase;
      letter-spacing: 0.8px;
    }
    .metric-card .sub {
      font-size: 11px;
      color: #6a6050;
      margin-top: 4px;
    }

    /* Section headers */
    h2 {
      font-family: 'Cinzel', serif;
      font-size: 22px;
      color: #daa520;
      margin: 50px 0 16px;
      padding-bottom: 8px;
      border-bottom: 1px solid rgba(218,165,32,0.1);
    }
    h3 {
      font-family: 'Inter', sans-serif;
      font-size: 16px;
      font-weight: 600;
      color: #c4a24e;
      margin: 28px 0 10px;
    }
    p, li { font-size: 15px; margin-bottom: 12px; }
    ul, ol { padding-left: 22px; margin-bottom: 16px; }
    li { margin-bottom: 6px; }
    a { color: #daa520; }
    a:hover { color: #ffd700; }
    strong { color: #e0c870; }

    /* Collapsible sections */
    .collapsible {
      border: 1px solid rgba(218,165,32,0.1);
      border-radius: 10px;
      margin: 12px 0;
      overflow: hidden;
      transition: border-color 0.3s;
    }
    .collapsible:hover { border-color: rgba(218,165,32,0.25); }
    .collapsible-header {
      padding: 14px 20px;
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: space-between;
      background: rgba(218,165,32,0.04);
      user-select: none;
      transition: background 0.2s;
    }
    .collapsible-header:hover { background: rgba(218,165,32,0.08); }
    .collapsible-header h3 { margin: 0; font-size: 15px; }
    .collapsible-arrow {
      color: #daa520;
      font-size: 14px;
      transition: transform 0.3s;
    }
    .collapsible.open .collapsible-arrow { transform: rotate(180deg); }
    .collapsible-body {
      max-height: 0;
      overflow: hidden;
      transition: max-height 0.4s ease, padding 0.3s ease;
      padding: 0 20px;
    }
    .collapsible.open .collapsible-body {
      max-height: 2000px;
      padding: 16px 20px 20px;
    }

    /* Comparison bar */
    .comparison-bar {
      display: flex;
      align-items: center;
      margin: 8px 0;
      gap: 10px;
    }
    .comparison-bar .bar-label {
      font-size: 12px;
      color: #8a7e60;
      min-width: 140px;
      text-align: right;
    }
    .comparison-bar .bar-track {
      flex: 1;
      height: 24px;
      background: rgba(218,165,32,0.06);
      border-radius: 6px;
      overflow: hidden;
      position: relative;
    }
    .comparison-bar .bar-fill {
      height: 100%;
      border-radius: 6px;
      background: linear-gradient(90deg, #b8860b, #daa520);
      transition: width 1.2s ease;
      display: flex;
      align-items: center;
      justify-content: flex-end;
      padding-right: 8px;
    }
    .comparison-bar .bar-fill.highlight {
      background: linear-gradient(90deg, #daa520, #ffd700);
    }
    .comparison-bar .bar-value {
      font-size: 11px;
      font-weight: 700;
      color: #0d0a06;
    }

    /* Pipeline diagram */
    .pipeline {
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 0;
      margin: 24px 0;
    }
    .pipeline-step {
      background: rgba(218,165,32,0.06);
      border: 1px solid rgba(218,165,32,0.15);
      border-radius: 10px;
      padding: 12px 24px;
      text-align: center;
      width: 100%;
      max-width: 400px;
      font-size: 14px;
      transition: border-color 0.3s, background 0.3s;
    }
    .pipeline-step:hover {
      border-color: rgba(218,165,32,0.35);
      background: rgba(218,165,32,0.1);
    }
    .pipeline-step strong { color: #daa520; }
    .pipeline-arrow {
      color: #5a5040;
      font-size: 18px;
      line-height: 1;
      padding: 2px 0;
    }

    /* Kappa interpretation inline */
    .kappa-scale {
      display: flex;
      gap: 0;
      margin: 16px 0;
      border-radius: 8px;
      overflow: hidden;
    }
    .kappa-segment {
      flex: 1;
      padding: 8px 4px;
      text-align: center;
      font-size: 10px;
      font-weight: 600;
      border-right: 1px solid rgba(13,10,6,0.4);
      transition: transform 0.2s;
    }
    .kappa-segment:last-child { border-right: none; }
    .kappa-segment:hover { transform: scaleY(1.08); }
    .kappa-segment .range { font-size: 9px; font-weight: 400; opacity: 0.8; display: block; margin-top: 2px; }
    .k-slight { background: rgba(180,80,80,0.3); color: #e8a0a0; }
    .k-fair { background: rgba(200,140,60,0.3); color: #e8c080; }
    .k-moderate { background: rgba(200,180,60,0.3); color: #e8d880; }
    .k-substantial { background: rgba(100,180,80,0.4); color: #c0e8a0; border: 2px solid rgba(100,180,80,0.6); }
    .k-perfect { background: rgba(60,160,120,0.3); color: #a0e8c0; }

    /* Unanimity visualization */
    .unanimity-viz {
      display: flex;
      gap: 20px;
      margin: 20px 0;
      justify-content: center;
    }
    .unanimity-block {
      text-align: center;
      padding: 20px;
      border-radius: 12px;
      flex: 1;
      max-width: 220px;
    }
    .unanimity-block.unanimous {
      background: rgba(100,180,80,0.08);
      border: 1px solid rgba(100,180,80,0.2);
    }
    .unanimity-block.split {
      background: rgba(200,140,60,0.08);
      border: 1px solid rgba(200,140,60,0.2);
    }
    .unanimity-block .pct {
      font-family: 'Cinzel', serif;
      font-size: 32px;
      font-weight: 900;
    }
    .unanimity-block.unanimous .pct { color: #7cd87c; }
    .unanimity-block.split .pct { color: #e8b060; }
    .unanimity-block .desc {
      font-size: 12px;
      color: #8a7e60;
      margin-top: 4px;
    }
    .unanimity-block .cases {
      font-size: 11px;
      color: #6a6050;
      margin-top: 2px;
    }

    /* CTA */
    .cta-section {
      text-align: center;
      margin: 50px 0 20px;
      padding: 30px;
      border: 1px solid rgba(218,165,32,0.15);
      border-radius: 14px;
      background: rgba(218,165,32,0.04);
    }
    .cta-btn {
      display: inline-block;
      padding: 12px 32px;
      background: linear-gradient(135deg, #daa520, #b8860b);
      color: #0d0a06;
      font-family: 'Cinzel', serif;
      font-weight: 700;
      font-size: 14px;
      text-decoration: none;
      border-radius: 8px;
      transition: opacity 0.2s, transform 0.2s;
      margin: 8px;
    }
    .cta-btn:hover { opacity: 0.85; transform: translateY(-1px); }
    .cta-btn.secondary {
      background: transparent;
      border: 1px solid rgba(218,165,32,0.3);
      color: #daa520;
    }

    /* Table */
    .data-table {
      width: 100%;
      border-collapse: collapse;
      margin: 16px 0;
      font-size: 13px;
    }
    .data-table th {
      text-align: left;
      padding: 10px 12px;
      border-bottom: 2px solid rgba(218,165,32,0.2);
      color: #daa520;
      font-weight: 600;
      font-size: 12px;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }
    .data-table td {
      padding: 8px 12px;
      border-bottom: 1px solid rgba(218,165,32,0.06);
      color: #b8a888;
    }
    .data-table tr:hover td { background: rgba(218,165,32,0.03); }
    .data-table .highlight-row td { color: #ffd700; font-weight: 600; }

    /* Footer */
    .footer-mini {
      border-top: 1px solid rgba(218,165,32,0.15);
      padding: 24px;
      text-align: center;
      font-size: 13px;
      color: #5a5040;
    }
    .footer-mini a { color: #8a7e60; text-decoration: none; margin: 0 10px; }
    .footer-mini a:hover { color: #daa520; }

    /* Responsive */
    @media (max-width: 600px) {
      h1 { font-size: 22px; }
      .metrics-grid { grid-template-columns: 1fr 1fr; }
      .metric-card .value { font-size: 28px; }
      .unanimity-viz { flex-direction: column; align-items: center; }
      .comparison-bar .bar-label { min-width: 100px; font-size: 11px; }
      .kappa-scale { flex-wrap: wrap; }
      .kappa-segment { flex: 1 1 30%; }
    }
  </style>
</head>
<body>

  <!-- Top Bar -->
  <div class="top-bar">
    <a href="https://agent-clash.ai" class="logo">Agent Clash</a>
    <div class="nav-links">
      <a href="/about.html">About</a>
      <a href="/paper.html" style="color:#daa520;">Research</a>
    </div>
  </div>

  <div class="container">

    <!-- ===== HERO ===== -->
    <div class="paper-hero">
      <div class="badge">Research Paper</div>
      <h1>Multi-Agent Judging for LLM Evaluation</h1>
      <p style="font-size:15px; color:#b0a080; margin-bottom:16px; line-height:1.5;">Concordance with Human Preferences Across Capability Gaps</p>
      <p class="author">Anthony Boisbouvier &mdash; <a href="https://agent-clash.ai">agent-clash.ai</a></p>
      <p class="venue">Submitted to DMLR (Data-centric Machine Learning Research), 2026</p>
    </div>

    <!-- ===== BOTTOM LINE ===== -->
    <div class="bottom-line">
      <p>A <strong>panel of 3 AI judges</strong> agrees with human evaluators <strong>88% of the time</strong> when comparing models of different quality levels, and <strong>76%</strong> when comparing top-tier models that are closely matched &mdash; <strong>on par with how often human evaluators agree with each other</strong>. All of this at a cost of ~$0.14 per evaluation.</p>
    </div>

    <!-- ===== KEY METRICS ===== -->
    <div class="metrics-grid">
      <div class="metric-card">
        <div class="value">88.0%</div>
        <div class="label">Agreement with Humans</div>
        <div class="sub">On models with large quality gaps</div>
      </div>
      <div class="metric-card">
        <div class="value">76.0%</div>
        <div class="label">On Top-Tier Models</div>
        <div class="sub">When even humans struggle to agree</div>
      </div>
      <div class="metric-card">
        <div class="value">91.0%</div>
        <div class="label">Reproducibility</div>
        <div class="sub">Same results when you run it again</div>
      </div>
      <div class="metric-card">
        <div class="value">$0.14</div>
        <div class="label">Per Evaluation</div>
        <div class="sub">360 evaluations for ~$51</div>
      </div>
    </div>

    <!-- ===== HOW IT WORKS ===== -->
    <h2>How It Works</h2>
    <p>Agent Clash evaluates candidate models through a <strong>6-stage pipeline</strong> designed for blind, reproducible, multi-judge evaluation:</p>

    <div class="pipeline">
      <div class="pipeline-step"><strong>1.</strong> User submits a prompt</div>
      <div class="pipeline-arrow">&#9660;</div>
      <div class="pipeline-step"><strong>2.</strong> Generate K responses via OpenRouter</div>
      <div class="pipeline-arrow">&#9660;</div>
      <div class="pipeline-step"><strong>3.</strong> Anonymize &mdash; strip IDs, shuffle, label A0&hellip;AK</div>
      <div class="pipeline-arrow">&#9660;</div>
      <div class="pipeline-step"><strong>4.</strong> Dynamic Criteria &mdash; 3-5 task-specific rubrics</div>
      <div class="pipeline-arrow">&#9660;</div>
      <div class="pipeline-step"><strong>5.</strong> Parallel Judging &mdash; GPT-5.2-pro, Claude Opus 4.5, Gemini 2.5 Pro</div>
      <div class="pipeline-arrow">&#9660;</div>
      <div class="pipeline-step"><strong>6.</strong> Aggregate via Borda Count &rarr; Winner + Confidence</div>
    </div>

    <!-- ===== FOUR CONTRIBUTIONS ===== -->
    <h2>Four Contributions</h2>

    <!-- Contribution 1 -->
    <div class="collapsible open" onclick="this.classList.toggle('open')">
      <div class="collapsible-header">
        <h3>1. AI Judges Are More Accurate When the Gap Between Models Is Larger</h3>
        <span class="collapsible-arrow">&#9660;</span>
      </div>
      <div class="collapsible-body">
        <p>We tested on two well-known benchmarks: <strong>MT-Bench</strong> (a standard dataset where models vary widely in quality) and <strong>Chatbot Arena</strong> (real-world user conversations where top models are very close in ability). When models differ clearly, our AI judges agree with human evaluators <strong>88.0%</strong> of the time &mdash; better than a single GPT-4 judge (85%) and even better than how often two human evaluators agree with each other (81%). When the task is harder (top-tier models that are nearly equal), agreement is <strong>76.0%</strong>, which falls right in the range where even human experts disagree (72-83%).</p>

        <h3 style="margin-top:20px;">Comparison with Prior Work</h3>
        <div style="margin: 12px 0;">
          <div class="comparison-bar">
            <span class="bar-label">Human vs Human</span>
            <div class="bar-track"><div class="bar-fill" style="width:81%"><span class="bar-value">81%</span></div></div>
          </div>
          <div class="comparison-bar">
            <span class="bar-label">GPT-4 single judge</span>
            <div class="bar-track"><div class="bar-fill" style="width:85%"><span class="bar-value">85%</span></div></div>
          </div>
          <div class="comparison-bar">
            <span class="bar-label">Agent Clash (MT-Bench)</span>
            <div class="bar-track"><div class="bar-fill highlight" style="width:88%"><span class="bar-value">88%</span></div></div>
          </div>
          <div class="comparison-bar">
            <span class="bar-label">Crowd-Expert (Arena)</span>
            <div class="bar-track"><div class="bar-fill" style="width:77.5%"><span class="bar-value">72-83%</span></div></div>
          </div>
          <div class="comparison-bar">
            <span class="bar-label">Agent Clash (Arena)</span>
            <div class="bar-track"><div class="bar-fill highlight" style="width:76%"><span class="bar-value">76%</span></div></div>
          </div>
        </div>

        <h3>Cohen's &kappa; &mdash; A Standard Measure of Agreement</h3>
        <div class="kappa-scale">
          <div class="kappa-segment k-slight">Slight<span class="range">&lt; 0.20</span></div>
          <div class="kappa-segment k-fair">Fair<span class="range">0.21-0.40</span></div>
          <div class="kappa-segment k-moderate">Moderate<span class="range">0.41-0.60</span></div>
          <div class="kappa-segment k-substantial">Substantial<span class="range">0.61-0.80</span></div>
          <div class="kappa-segment k-perfect">Almost Perfect<span class="range">0.81-1.00</span></div>
        </div>
        <p style="font-size:13px; color:#8a7e60;">Cohen's &kappa; (kappa) measures agreement beyond random chance &mdash; it's the gold standard in research for comparing raters. Our &kappa; = 0.760 on models with large quality gaps falls in <strong style="color:#c0e8a0;">Substantial Agreement</strong>. On closely-matched top models, &kappa; = 0.520 is <strong style="color:#e8d880;">Moderate</strong> &mdash; expected since even human experts disagree more on close calls.</p>
      </div>
    </div>

    <!-- Contribution 2 -->
    <div class="collapsible" onclick="this.classList.toggle('open')">
      <div class="collapsible-header">
        <h3>2. Run It Twice, Get the Same Answer (91% of the Time)</h3>
        <span class="collapsible-arrow">&#9660;</span>
      </div>
      <div class="collapsible-body">
        <p>We ran the exact same evaluation twice independently. The results matched <strong>91.0%</strong> of the time &mdash; meaning the system is highly reproducible, not random. A third run on 60 cases confirmed:</p>
        <ul>
          <li><strong>95%</strong> of persistent "errors" (cases where AI judges disagree with humans) are consistent across all runs &mdash; they're genuine hard cases, not random mistakes</li>
          <li>Only <strong>9%</strong> of evaluations are truly unpredictable (like a coin flip)</li>
          <li>Running the evaluation 3 times and taking a majority vote does <strong>not</strong> improve accuracy &mdash; confirming that the remaining disagreements are inherently subjective, not fixable with more runs</li>
        </ul>
        <p style="font-size:13px; color:#8a7e60;">Breakdown: 72% consistently correct, 19% consistent disagreements with humans (genuinely ambiguous cases), 9% random noise.</p>
      </div>
    </div>

    <!-- Contribution 3 -->
    <div class="collapsible" onclick="this.classList.toggle('open')">
      <div class="collapsible-header">
        <h3>3. When All 3 Judges Agree, They're Almost Always Right</h3>
        <span class="collapsible-arrow">&#9660;</span>
      </div>
      <div class="collapsible-body">
        <p>Across all 239 evaluations, we compared what happens when all 3 AI judges agree (unanimous) vs. when they split 2-to-1:</p>
        <div class="unanimity-viz">
          <div class="unanimity-block unanimous">
            <div class="pct">84.9%</div>
            <div class="desc">Unanimous (3-0)</div>
            <div class="cases">192 evals &middot; 80% of cases</div>
          </div>
          <div class="unanimity-block split">
            <div class="pct">63.8%</div>
            <div class="desc">Split (2-1)</div>
            <div class="cases">47 evals &middot; 20% of cases</div>
          </div>
        </div>
        <p>The <strong>21-point accuracy gap</strong> between unanimous and split decisions gives you a built-in confidence score that a single judge can never provide. <strong>In practice:</strong> when all 3 judges agree, you can trust the result. When they disagree, it flags the evaluation for human review &mdash; saving you time by only involving humans where it actually matters.</p>
      </div>
    </div>

    <!-- Contribution 4 -->
    <div class="collapsible" onclick="this.classList.toggle('open')">
      <div class="collapsible-header">
        <h3>4. AI Judges Don't Cheat &mdash; No Bias, No Self-Favoritism</h3>
        <span class="collapsible-arrow">&#9660;</span>
      </div>
      <div class="collapsible-body">
        <p><strong>No bias toward "better" models:</strong> A common concern is that AI judges might systematically favor well-known or higher-ranked models. We tested this: when models are closely matched, AI judges picked the supposedly "stronger" model only 45.8% of the time in disagreements &mdash; essentially random. They judge based on actual response quality, not reputation.</p>
        <p><strong>No self-favoritism:</strong> Can GPT fairly judge GPT's own output? We tested 252 cases where an AI model was judging its own responses (without knowing it). Models ranked themselves first only 52.8% of the time vs. 59.9% expected &mdash; actually showing a slight <em>anti</em>-self-bias. <strong>Bottom line:</strong> under blind conditions, AI judges don't favor themselves.</p>
      </div>
    </div>

    <!-- ===== ABLATION ===== -->
    <h2>Why 3 Judges Instead of 1?</h2>
    <p>Using a panel of 3 different AI judges (GPT, Claude, Gemini) matches the accuracy of the single best judge &mdash; <strong>without you having to guess which judge is best for your specific task</strong>:</p>

    <table class="data-table">
      <thead>
        <tr><th>Judge</th><th>MT-Bench</th><th>Arena</th><th>Pooled</th></tr>
      </thead>
      <tbody>
        <tr><td>GPT-5.2-pro</td><td>88.9%</td><td>74.2%</td><td>81.8%</td></tr>
        <tr><td>Claude Opus 4.5</td><td>87.9%</td><td>76.3%</td><td>82.3%</td></tr>
        <tr><td>Gemini 2.5 Pro</td><td>83.8%</td><td>69.9%</td><td>77.1%</td></tr>
        <tr class="highlight-row"><td><strong>Panel (2-of-3)</strong></td><td><strong>88.9%</strong></td><td><strong>75.3%</strong></td><td><strong>82.3%</strong></td></tr>
      </tbody>
    </table>

    <p style="font-size:13px; color:#8a7e60;"><strong>Key insight:</strong> The best individual judge changes depending on the task (GPT wins on one dataset, Claude on another). You can't know in advance which one to trust. The 3-judge panel <strong>always</strong> matches the best one &mdash; and protects you from accidentally relying on the worst (+5.2 points of safety margin).</p>

    <!-- ===== CROSS-EXPERIMENT ===== -->
    <h2>Cross-Experiment Summary</h2>

    <table class="data-table">
      <thead>
        <tr><th>Metric</th><th>Different quality levels</th><th>Top-tier models only</th></tr>
      </thead>
      <tbody>
        <tr><td>Models tested</td><td>6 (large quality gaps)</td><td>25 (best models, very close)</td></tr>
        <tr><td>Total evaluations</td><td>100</td><td>260 (3 independent runs)</td></tr>
        <tr><td>Agreement with humans</td><td><strong>88.0%</strong></td><td><strong>76.0%</strong></td></tr>
        <tr><td>Cohen's &kappa; (agreement score)</td><td>0.760 (Substantial)</td><td>0.520 (Moderate)</td></tr>
        <tr><td>Human-human agreement</td><td>81% (expert)</td><td>72-83% (crowd)</td></tr>
        <tr><td>Cost/eval</td><td>$0.129</td><td>$0.149</td></tr>
        <tr><td>Test-retest</td><td>&mdash;</td><td>91.0%</td></tr>
      </tbody>
    </table>
    <p style="font-size:13px; color:#8a7e60;">The 12-point drop from 88% to 76% is expected and statistically confirmed: it's harder to pick a winner when all models are very good. This mirrors exactly what happens with human evaluators &mdash; they also agree less on close calls.</p>

    <!-- ===== CTA ===== -->
    <div class="cta-section">
      <h2 style="border-bottom:none; margin-top:0;">Try It Yourself</h2>
      <p style="font-size:15px; margin-bottom:16px;">Agent Clash is free and open. Bring your own API key, run your own benchmarks.</p>
      <a href="https://agent-clash.ai" class="cta-btn">Launch Agent Clash</a>
      <a href="https://github.com/anthonyboisbouvier-paris/agent-clash-paper" class="cta-btn secondary" target="_blank" rel="noopener">View on GitHub</a>
    </div>

    <!-- ===== LINKS ===== -->
    <h2>Resources</h2>
    <ul>
      <li><strong>Full paper (PDF):</strong> <a href="https://github.com/anthonyboisbouvier-paris/agent-clash-paper" target="_blank" rel="noopener">GitHub Repository</a> &mdash; includes LaTeX source, datasets, raw results</li>
      <li><strong>Platform:</strong> <a href="https://agent-clash.ai">agent-clash.ai</a> &mdash; free, no installation required</li>
      <li><strong>Methodology:</strong> <a href="/about.html">About Agent Clash</a></li>
    </ul>

  </div>

  <!-- Footer -->
  <div class="footer-mini">
    <a href="https://agent-clash.ai">Home</a>
    <a href="/about.html">About</a>
    <a href="/paper.html">Research</a>
    <a href="/privacy.html">Privacy</a>
    <a href="/terms.html">Terms</a>
  </div>

</body>
</html>
